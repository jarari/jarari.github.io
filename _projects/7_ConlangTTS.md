---
name: Text-To-Speech Synthesis for Constructed Languages
tools: [Python]
image: assets/conlangtts.png
description: 대학 캡스톤 프로젝트 - 인공어 TTS
---

# Text-To-Speech Synthesis for <br> Constructed Languages

<br>
![Conlang Flag](assets/conlang_symbol.png)
### Constructed Language (인공어) 란?
인공어는 자연적으로 형성된 한국어, 영어와 같은 언어가 아닌, 특정한 의도나 목적을 가지고 창작된 언어를 의미합니다. 프로그래밍에 사용되는 컴퓨터 언어도 이러한 인공어의 정의에는 부합하지만, 보통 인공어는 의사소통의 성격을 더 크게 가지고 있기 때문에 컴퓨터 언어와는 약간의 차이가 있습니다.<br>
<br>
인공어에는 크게 개인적으로 창작한 예술어와 세계 공용어를 목적으로 한 국제어 두 가지로 나뉩니다. 가장 이해하기 쉬운 예시로는, 스타트랙 시리즈의 클링온어, J.R.R 톨킨의 여러 작품에 등장하는 엘프어, 스카이림의 용언 (Thu'um) 등 창작물에서 우리가 살아가는 세상과는 이질적인 문명을 표현하기 위해 작가가 창작한 용어들이 인공어에 포함됩니다. 또한, 만인 평등과 세계 평화를 추구하기 위해 고안된 국제 공용어 에스페란토도 이러한 인공어에 포함됩니다.<br>
<br>
### 인공어를 위한 TTS 합성기 프로젝트의 목적

![Conlang TTS](assets/conlangtts.png)
본 프로젝트는 Oregon State University의 ([Patrick Donnelly](https://osucascades.edu/directory/patrick-donnelly)) 교수님께서 제안하신 프로젝트로, 자신만의 인공어를 창작, 공유하는 여러 커뮤니티를 돕기 위해 TTS를 제작하여보자는 취지로 시작되었습니다.<br>
<br>
잘 알려져있지는 않지만, 이러한 인공어를 직접 만들고 공유하거나 가르치는 다양한 사람들이 있습니다. 이 때 발생하는 가장 큰 문제는 아무래도 완전히 새로운 언어이고 텍스트 기반의 커뮤니티에서 공유되기 때문에 해당 언어를 말하는 방법을 전달하기가 쉽지 않다는 점입니다. 이러한 문제를 해결하기 위해, 전 세계 언어의 각종 언어들에 존재하는 발음들을 모아 phonetic database를 구축하고, 사용자가 본인의 언어에 대한 International Phonetic Alphabet (IPA) 맵을 제공하면 이에 해당하는 음성을 최대한 자연스럽게 합성하여 출력하여 제공하는 프로그램을 만들고자 하였습니다. 또한, 사용자의 편의를 위해 웹 프론트엔드와 디스코드 봇을 제공하기로 하였습니다.<br>
<br>
총 7명으로 구성된 팀으로 작업하였고, 저는 이 팀에서 phonetic database 와 주어진 IPA 맵을 바탕으로 최대한 자연스러운 음성을 낼 수 있는 phoneme (음소) 조합을 찾아 합성하는 백엔드 알고리즘을 담당하였습니다.<br>
<br>
### 협업 과정
해당 프로젝트가 진행된 기간이 아쉽게도 코로나의 여파가 가장 심했던 2023년이었기 때문에 저희는 매주 일정한 요일, 일정한 시간에 교수님과 줌으로 화상 대화를 진행하며 자료 조사, 역할 분담과 프로젝트 경과를 보고하였습니다. 매 회의는 구글 드라이브에 공유된 파워포인트에 내용을 기록하여 회의에 미처 참석하지 못한 인원들도 프로젝트 진행 상황과 공지를 전달 받을 수 있도록 하였고, Trello를 통해 현재 해야하는 업무, 각자 담당하고 있는 업무, 완료된 업무를 분류하여 트래킹할 수 있도록 구성하였습니다.<br>
<br>
### 어려웠던 점
{% include elements/figure.html image="projects/assets/heiga-zen-concat-synth.png" caption="출처: Generative Model-Based Text-to-Speech Synthesis(Heiga Zen, 2017)" %}
제가 사운드 엔지니어링과 음성학에 대한 지식이 전무했던 만큼, 이 프로젝트는 난관이 많았습니다.<br>
<br>
먼저, 저희는 음성을 음소 단위로 합성하여 데이터베이스를 구축하기 위해 eSpeak-NG 라는 오픈소스 TTS 엔진을 이용하였습니다. 해당 엔진을 이용하여 81개 언어의 스와데시 목록에 대한 음성을 모두 합성하고, 이를 sqlite3 데이터베이스와 wav 파일들로 저장하였습니다. 이러한 방식으로 만들어진 음성은 사람이 실제 말할 때의 발음에 비해 음소간의 전환이 자연스럽지 못하였고, 이를 음성 단어로 구성하기 위해 저는 diphone synthesis 같은 기술에 대한 논문들을 찾아보다 결국 단순한 concatenative synthesis를 변형하여 사용하기로 하였습니다.<br>
<br>
음성을 합성하기 위한 알고리즘은 결정하였지만, 81개 언어에 포함되어있는 방대한 양의 발음 중 어떤 것이 현재 IPA 맵과 입력된 단어들에 적절한지를 골라내는 것도 중요한 부분이었습니다. 저는 교수님께서 처음 제안하신대로 n-gram 모델을 이용하여 음절을 3부분, 2부분, 1부분으로 나눈 뒤, 음소 데이터베이스에서 해당 n-gram과 가장 일치율이 높은 음소들만 선택하는 알고리즘을 제작하였습니다. 이 방법은 결과물은 나왔지만 그렇게 자연스럽지는 못하였습니다. 그래서 추가적으로 n-gram 매칭에서 발견된 리스트를 모두 종합하여 출현 빈도수가 가장 높은 언어를 순차적으로 택하는 방식으로, 모든 음소가 최대한 같은 언어에서 이어지도록 등장하는 알고리즘을 작성하였습니다.<br>
<br>
이후에는 음소들을 합성하여 오디오 파일로 출력하는 부분이 있었는데, 이 파트는 다른 조원이 만든 뼈대에 서로 협의를 거쳐 음소의 특정 부분을 잘라내거나, 페이드 인/페이드 아웃 효과를 이용하여 조금 더 자연스럽게 출력되도록 다듬었습니다.
<br>
### 결과물
모든 팀원들이 각자 맡은 파트를 잘 해주고, 마지막까지 따로 모여서 검수와 빌드까지 마친 결과 저희는 순조롭게 오리건 주립 대학교 엔지니어링 엑스포에서 저희 프로젝트에 대해 방문객분들께 설명드리고, 체험시켜드릴 수 있는 기회를 가졌습니다. 이후에 홈페이지에도 호스팅 될 예정이었으나, 아쉽게도 Soundbendor Lab 홈페이지가 리뉴얼되면서 저희 작업물이 더이상 호스팅 되지는 않는 것 같습니다.